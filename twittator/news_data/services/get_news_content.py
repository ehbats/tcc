import urllib.request as req
from urllib.request import Request, urlopen

from bs4 import BeautifulSoup
from bs4.element import Comment

class GetNewsContent:
    
    def get_original_content_url(self, raw_html):
        """
        When accessing a url from the google news parsed data, it will get you to an url that redirects you to the original 
        news url. It shows on the screen the url of the website it is going to redirect you. This method gets that url to 
        access the website more quickly, and avoid any errors generated by searching the news data before the original url
        is actually accessed.

        Receives the html of the redirect page and returns the original news url.
        """
        soup = BeautifulSoup(raw_html, 'html.parser')
        texts = soup.findAll(string = True)
        filtered_html = filter(self.tag_visible, texts)  
        parsed_texts = u" ".join(t.strip() for t in filtered_html)
        news_url = parsed_texts.split(' ')[1]

        return news_url
    
    def get_original_content_text(self, original_url):
        """
        Gets all of the text contained inside <p> tags that are inside the <article> tag.
        """
        fake_request = self.generate_fake_request(original_url)
        request = urlopen(fake_request).read()
        soup = BeautifulSoup(request, 'html.parser')
        texts = soup.find('article')
        p_list = []
        if texts != None:
            for p in texts.select("p"):
                p_list.append(p.text)

        return p_list

    def tag_visible(self, element):
        """
        Gets all of the elements that are visible to the user.
        """
        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:
            return False
        if isinstance(element, Comment):
            return False
        return True
    
    def tag_p(self, element):
        """
        Gets all of the elements that are inside <p> tags.
        """
        if element.parent.name in ['p', ['body']]:
            return True
        else:
            return False
        
    def tag_article(self, element):
        """
        Gets all of the elements that are inside the <article> tags.
        """
        if element.parent.name in ['article']:
            return True
        else:
            return False
    
    def generate_fake_request(
            self, 
            url: str
            ):
        """
        Generates a fake request to avoid captchas
        """
        return Request(
            url = url,
            headers = {'User-Agent': 'Mozilla/5.0'}
        )
    
    def filter_relevant_content(
            self, 
            p_list: list
            ):
        """
        This function avoids getting unwanted text such as ads, related news or "share" buttons. It fetches
        only the first 3 <p> tags that have more than 120 characters inside the <article> tag. This is 
        an effective way to get only the first three paragraphs of the news. 
        """
        filtered_p_list = []

        for p_text in p_list:
            if len(p_text) > 120:
                if len(filtered_p_list) < 3:
                    filtered_p_list.append(p_text)
                else:
                    break

        return filtered_p_list
    
    def get_content(
            self, 
            url: str
            ):
        """
        This function articulates all of this class's methods to get the first three paragraphs from a google news rss observation.
        """
        fake_request = self.generate_fake_request(url)
        
        request = urlopen(fake_request).read()
        
        original_url = self.get_original_content_url(request)

        p_list = self.get_original_content_text(original_url)

        final_p = self.filter_relevant_content(p_list)

        return (final_p, original_url)
